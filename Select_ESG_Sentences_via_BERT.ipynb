{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ZFgdgkcYM_5S","outputId":"f516701f-e179-4d7a-acc7-d4d9ee38fad6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: gensim in c:\\users\\alice\\anaconda3\\lib\\site-packages (4.3.1)\n","Requirement already satisfied: scipy>=1.7.0 in c:\\users\\alice\\anaconda3\\lib\\site-packages (from gensim) (1.10.1)\n","Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\alice\\anaconda3\\lib\\site-packages (from gensim) (6.3.0)\n","Requirement already satisfied: numpy>=1.18.5 in c:\\users\\alice\\anaconda3\\lib\\site-packages (from gensim) (1.22.3)\n","Requirement already satisfied: tabulate in c:\\users\\alice\\anaconda3\\lib\\site-packages (0.9.0)\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\alice\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["from PyPDF2 import PdfReader\n","import numpy as np\n","import pandas as pd\n","import nltk\n","import string\n","import re\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","!pip install gensim\n","from gensim.parsing.preprocessing import remove_stopwords\n","!pip install tabulate\n","from tabulate import tabulate\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from transformers import pipeline\n","import nltk\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TiFjd6seM_5a"},"outputs":[],"source":["stop_words = set(stopwords.words(\"english\"))\n","\n","cleaner = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jTpRaUMLM_5c"},"outputs":[],"source":["# functions\n","def extract(pdf_file:str):\n","    file_read = PdfReader(pdf_file)\n","    pdf_text=\"\"\n","    for page in file_read.pages:\n","        content = page.extract_text()\n","        pdf_text += content\n","    return pdf_text \n","\n","def cleanhtml(raw_html):\n","  cleantext = re.sub(cleaner, '', raw_html)\n","  return cleantext\n","\n","def remove_numbers(text):\n","    result = re.sub(r'\\d+', '', text)\n","    return result\n","\n","def remove_punctuation(text):\n","    translator = str.maketrans('', '', string.punctuation)\n","    return text.translate(translator)\n","\n","def remove_whitespace(text):\n","    return  \" \".join(text.split())\n","\n","def cut_in_sliding_windowns(text):\n","    separations=[]\n","    text_to_words=word_tokenize(text)\n","    n=len(text_to_words)\n","    i=0\n","    while i<n:\n","        separations.append(i)\n","        i=i+200\n","    list_section=[]\n","    for i in separations:\n","        list_section.append(text_to_words[i:i+250])\n","    return list_section\n","\n","def count_esg(text):\n","    count_ecolo = 0\n","    count_social = 0 \n","    count_governance = 0 \n","    for word in word_tokenize(text):\n","        count_neg += 1 if word.upper() in neg else 0\n","        count_pos += 1 if word.upper() in pos else 0\n","    return (count_pos-count_neg)/(count_pos+count_neg)\n","\n","def select_esg_sentences(datafr):\n","    data_esg=datafr.copy()\n","    data_esg.drop(data_esg[data_esg['label'] == 'None'].index, inplace = True)\n","    return data_esg \n","#Bert doesn't work for sentences of more than 512 words; I just drop them\n","#the percentage of such 'too long' sentences is the last value of the returned array\n","\n","def bert_phrase(list):\n","    finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-esg',num_labels=4)\n","    tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-esg')\n","    nlp = pipeline(\"text-classification\", model=finbert, tokenizer=tokenizer)\n","    n = len(list)\n","    sent=[]\n","    label=[]\n","    score=[]\n","    if n>0: \n","        for sentence in list:\n","            if len(sentence)<512:\n","                result = nlp(sentence)\n","                sent.append(sentence)\n","                label.append(result[0]['label'])\n","                score.append(result[0]['score'])\n","                n=n-1\n","            else:\n","                n=n-1\n","    df_phrases = pd.DataFrame({'sentence':sent, 'label': label, 'score':score}, columns=['sentence', 'label', 'score'])\n","    return df_phrases \n","#Bert doesn't work for sentences of more than 512 words; I just drop them\n","#the percentage of such 'too long' sentences is the last value of the returned array"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zoKQ9kkvM_5f"},"outputs":[],"source":["x = extract('10-KFord2022.pdf')\n","x = cleanhtml(x)\n","x = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \" \", x)\n","x= x.lower()\n","x= remove_numbers(x)\n","x = x.split(\".\") \n","for i in range (len(x)):\n","    x[i] = re.sub(r'[^\\w]', ' ', x[i])\n","for phrase in x:\n","    if (len(phrase)<15):\n","        x.remove(phrase)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bSmQpFkYcph9","outputId":"1c56ab53-27de-4dcf-bf0c-8b7379a2d66f"},"outputs":[{"name":"stdout","output_type":"stream","text":["   united states securities and exchange commission washington  dc  form  k   annual report pursuant to section  or  d  of the securities exchange act of  for the fiscal year ended december    or   transition report pursuant to section  or  d  of the securities exchange act of  for the transition period from  __________ to __________ commission file number     ford motor company  exact name of registrant as specified in its charter  delaware    state of incorporation   i\n"]}],"source":["print(x[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wlXpB8hHcph9"},"outputs":[],"source":["bert_phrases = bert_phrase(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SSOum9Jlcph-","outputId":"322ec0e4-afa1-4dcb-eeb4-3eb1038c1aae"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>label</th>\n","      <th>score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>united states securities and exchange commi...</td>\n","      <td>None</td>\n","      <td>0.991014</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>employer identification no</td>\n","      <td>None</td>\n","      <td>0.628254</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>one american road dearborn michigan   add...</td>\n","      <td>None</td>\n","      <td>0.981163</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>per share f new york stock exchange</td>\n","      <td>None</td>\n","      <td>0.983001</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>notes due june    fprb new york stock exchange</td>\n","      <td>None</td>\n","      <td>0.994293</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3880</th>\n","      <td>for the year ended   includes     million rel...</td>\n","      <td>None</td>\n","      <td>0.996267</td>\n","    </tr>\n","    <tr>\n","      <th>3881</th>\n","      <td>b accounts receivable deemed to be uncollect...</td>\n","      <td>None</td>\n","      <td>0.995085</td>\n","    </tr>\n","    <tr>\n","      <th>3882</th>\n","      <td>c net change in inventory allowances  includ...</td>\n","      <td>None</td>\n","      <td>0.995496</td>\n","    </tr>\n","    <tr>\n","      <th>3883</th>\n","      <td>d change in valuation allowance on deferre...</td>\n","      <td>None</td>\n","      <td>0.995705</td>\n","    </tr>\n","    <tr>\n","      <th>3884</th>\n","      <td></td>\n","      <td>None</td>\n","      <td>0.945206</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3885 rows × 3 columns</p>\n","</div>"],"text/plain":["                                               sentence label     score\n","0        united states securities and exchange commi...  None  0.991014\n","1                            employer identification no  None  0.628254\n","2          one american road dearborn michigan   add...  None  0.981163\n","3                  per share f new york stock exchange   None  0.983001\n","4       notes due june    fprb new york stock exchange   None  0.994293\n","...                                                 ...   ...       ...\n","3880   for the year ended   includes     million rel...  None  0.996267\n","3881    b accounts receivable deemed to be uncollect...  None  0.995085\n","3882    c net change in inventory allowances  includ...  None  0.995496\n","3883      d change in valuation allowance on deferre...  None  0.995705\n","3884                                                     None  0.945206\n","\n","[3885 rows x 3 columns]"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["bert_phrases"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ds4JjcLLM_5h"},"outputs":[],"source":["bert_phrases.to_csv('Ford2022_Select_ESG_Sentences_via_BERT.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V8AmmLqccph_"},"outputs":[],"source":["df = pd.read_csv('Ford2022_Select_ESG_Sentences_via_BERT.csv')\n","df=df.drop('Unnamed: 0',axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mNn0Y4C5cpiA"},"outputs":[],"source":["esg_sentences=select_esg_sentences(df)\n","esg_sentences.to_csv('Ford2022_onlyEsgSentences_Select_ESG_Sentences_via_BERT.csv')"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"nbformat":4,"nbformat_minor":0}