{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ZFgdgkcYM_5S","outputId":"f516701f-e179-4d7a-acc7-d4d9ee38fad6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: gensim in c:\\users\\alice\\anaconda3\\lib\\site-packages (4.3.1)\n","Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\alice\\anaconda3\\lib\\site-packages (from gensim) (6.3.0)\n","Requirement already satisfied: scipy>=1.7.0 in c:\\users\\alice\\anaconda3\\lib\\site-packages (from gensim) (1.10.1)\n","Requirement already satisfied: numpy>=1.18.5 in c:\\users\\alice\\anaconda3\\lib\\site-packages (from gensim) (1.22.3)\n","Requirement already satisfied: tabulate in c:\\users\\alice\\anaconda3\\lib\\site-packages (0.9.0)\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\alice\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["from PyPDF2 import PdfReader\n","import numpy as np\n","import pandas as pd\n","import nltk\n","import string\n","import re\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","!pip install gensim\n","from gensim.parsing.preprocessing import remove_stopwords\n","!pip install tabulate\n","from tabulate import tabulate\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from transformers import pipeline\n","import nltk\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TiFjd6seM_5a"},"outputs":[],"source":["# what we need to start \n","company_list = ['10-KFord.pdf', '10-KTesla.pdf', '10-KStandard_Motor_Products.pdf', '10-KRivian.pdf', '10-KMotorcar_Parts_America.pdf', \\\n","        '10-KMS.pdf', '10-KSilicon.pdf', '10-KJPMorgan.pdf', '10-KCityGroup.pdf', '10-KBankofAmerica.pdf',\\\n","        '10-KChevron.pdf', '10-KConoco.pdf', '10-KGeneral_Electric.pdf', '10-KPetrogas.pdf', '10-KRoyal_Energy.pdf',\\\n","        '10-KBeyond_Meat.pdf', '10-KCampbell.pdf', '10-KHershey.pdf', '10-KKellogg.pdf', '10-KCMcDonalds.pdf']\n","\n","# file_list = ['file_1.txt', 'file_2.txt', 'file_3.txt', 'file_4.txt', 'file_5.txt', 'file_6.txt',\\\n","#              'file_7.txt', 'file_8.txt', 'file_9.txt', 'file_10.txt', 'file_11.txt', 'file_12.txt',\\\n","#                  'file_13.txt', 'file_14.txt', 'file_15.txt', 'file_16.txt', 'file_17.txt', \\\n","#                      'file_18.txt', 'file_19.txt', 'file_20.txt' ]\n","\n","social = pd.read_excel('social.xlsx')\n","environment = pd.read_excel('environment.xlsx')\n","economy = pd.read_excel('economy.xlsx')\n","social_words = social['social words'].tolist()\n","environment_words = environment['environment words'].tolist()\n","economy_words = economy['economy words'].tolist()\n","words_list = social_words + environment_words + economy_words\n","\n","stop_words = set(stopwords.words(\"english\"))\n","\n","cleaner = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n","\n","# LM_dictionary = pd.read_excel('LM_MasterDictionary.xlsx', dtype={'Word': str})\n","# neg = LM_dictionary[LM_dictionary['Negative']>0]['Word'].tolist()\n","# pos = LM_dictionary[LM_dictionary['Positive']>0]['Word'].tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jTpRaUMLM_5c"},"outputs":[],"source":["# functions\n","def extract(pdf_file:str):\n","    file_read = PdfReader(pdf_file)\n","    pdf_text=\"\"\n","    for page in file_read.pages:\n","        content = page.extract_text()\n","        pdf_text += content\n","    return pdf_text \n","\n","def cleanhtml(raw_html):\n","  cleantext = re.sub(cleaner, '', raw_html)\n","  return cleantext\n","\n","def remove_numbers(text):\n","    result = re.sub(r'\\d+', '', text)\n","    return result\n","\n","def remove_punctuation(text):\n","    translator = str.maketrans('', '', string.punctuation)\n","    return text.translate(translator)\n","\n","def remove_whitespace(text):\n","    return  \" \".join(text.split())\n","\n","def esg_percentage(text_file,list_of_words):\n","    count = 0\n","    text_split = text_file.split()\n","    n = len(text_split)\n","    for i in range(0,n):\n","        for word in list_of_words:\n","            if text_split[i]==word.lower(): \n","                count += 1 \n","            elif i<n-1 and text_split[i]+\" \"+text_split[i+1]==word.lower(): \n","                count += 1 \n","            elif i<n-2 and text_split[i]+\" \"+text_split[i+1]+\" \"+text_split[i+2]==word.lower():\n","                count += 1 \n","            elif i<n-3 and text_split[i]+\" \"+text_split[i+1]+\" \"+text_split[i+2]+\" \"+text_split[i+3]==word.lower():\n","                count += 1\n","    percentage_words = count/n\n","    return percentage_words\n","\n","def clean_list(list,list_of_words):\n","    new_list = []\n","    accepted = 0 \n","    total_number_phrases = len(list)\n","    for phrase in list:\n","        phrase = remove_punctuation(phrase) # remove punctuation\n","        phrase = re.sub(r'[^\\w]', ' ', phrase) # remove all character that are different from words\n","        words_list = word_tokenize(phrase) # create a list of words from the string phrase\n","        filtered_words_list = [word for word in words_list if word not in stop_words] # remove stopwords\n","        count = 0\n","        n = len(filtered_words_list)\n","        for i in range(0,n):\n","            for word in list_of_words:\n","                if filtered_words_list[i]==word.lower(): \n","                    count += 1 \n","                elif i<n-1 and filtered_words_list[i]+\" \"+filtered_words_list[i+1]==word.lower(): \n","                    count += 1 \n","                elif i<n-2 and filtered_words_list[i]+\" \"+filtered_words_list[i+1]+\" \"+filtered_words_list[i+2]==word.lower():\n","                    count += 1 \n","                elif i<n-3 and filtered_words_list[i]+\" \"+filtered_words_list[i+1]+\" \"+filtered_words_list[i+2]+\" \"+filtered_words_list[i+3]==word.lower():\n","                    count += 1\n","        if count != 0: \n","            accepted += 1\n","            phrase = ' '.join(filtered_words_list) # create again the string \n","            new_list = new_list + [phrase]\n","    return new_list, accepted/total_number_phrases\n","\n","def count_neg_pos(text):\n","    count_neg = 0\n","    count_pos = 0 \n","    for word in word_tokenize(text):\n","        count_neg += 1 if word.upper() in neg else 0\n","        count_pos += 1 if word.upper() in pos else 0\n","    return (count_pos-count_neg)/(count_pos+count_neg)\n","\n","def bert_phrase(list):\n","    finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n","    tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n","    nlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)\n","\n","    n = len(list)\n","    count_pos = 0\n","    count_neg = 0 \n","    count_neut = 0 \n","    score_pos = 0\n","    score_neg = 0\n","    score_neut = 0\n","    compound = 0 \n","    if n>0: \n","        for sentence in list:\n","            if len(sentence)<512:\n","                result = nlp(sentence)\n","                label_result=result[0]['label']\n","                count_pos = ((label_result=='Positive')+count_pos)\n","                count_neg =  ((label_result=='Negative')+count_neg)\n","                count_neut =  ((label_result=='Neutral')+count_neut)\n","            else:\n","                n=n-1\n","        compound=(count_pos*1 + count_neg *(-1))/n\n","        count_pos=count_pos/n\n","        count_neg=count_neg/n\n","        count_neut=count_neut/n\n","    array = np.array([compound, count_pos, count_neg, count_neut, n/len(list)])\n","    return array \n","#Bert doesn't work for sentences of more than 512 words; I just drop them\n","#the percentage of such 'too long' sentences is the last value of the returned array"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zoKQ9kkvM_5f"},"outputs":[],"source":["list_companies = ['Ford', 'Tesla', 'Standard_Motor_Products', 'Rivian', 'Motorcar_Parts_America', 'MS', 'Silicon', 'JPMorgan', 'CityGroup', 'BankofAmerica',\\\n","        'Chevron', 'Conoco', 'General_Electric', 'Petrogas', 'Royal_Energy','Beyond_Meat', 'Campbell', 'Hershey', 'Kellogg', 'McDonalds']\n","df_phrases = pd.DataFrame(columns=['average_compound','perc_pos','perc_neg','perc_neut', 'perc_esg_phrases'], index=list_companies)\n","df_document = pd.DataFrame(columns=['sentiment'], index=list_companies)\n","count = 0\n","for i in company_list: \n","    x = extract(i)\n","    x = cleanhtml(x)\n","    x = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \" \", x)\n","    x= x.lower()\n","    x= remove_numbers(x)\n","    x = x.split(\".\")\n","    x,acceptance_rate = clean_list(x,words_list)\n","    bert_phrases = bert_phrase(x)\n","    df_phrases.loc[list_companies[count]] = pd.Series({'average_compound': bert_phrases[0],'perc_pos': bert_phrases[1], 'perc_neg': bert_phrases[2], 'perc_neut': bert_phrases[3], 'perc_esg_phrases': acceptance_rate})\n","    count += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TRGIWOg5M_5g","outputId":"a24a912d-2fd8-4ae9-9635-f9ae6ab3be52"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>average_compound</th>\n","      <th>perc_pos</th>\n","      <th>perc_neg</th>\n","      <th>perc_neut</th>\n","      <th>perc_esg_phrases</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Ford</th>\n","      <td>-0.07393</td>\n","      <td>0.107004</td>\n","      <td>0.180934</td>\n","      <td>0.712062</td>\n","      <td>0.110452</td>\n","    </tr>\n","    <tr>\n","      <th>Tesla</th>\n","      <td>-0.005988</td>\n","      <td>0.164671</td>\n","      <td>0.170659</td>\n","      <td>0.664671</td>\n","      <td>0.118833</td>\n","    </tr>\n","    <tr>\n","      <th>Standard_Motor_Products</th>\n","      <td>-0.024814</td>\n","      <td>0.133995</td>\n","      <td>0.158809</td>\n","      <td>0.707196</td>\n","      <td>0.158033</td>\n","    </tr>\n","    <tr>\n","      <th>Rivian</th>\n","      <td>-0.131737</td>\n","      <td>0.155689</td>\n","      <td>0.287425</td>\n","      <td>0.556886</td>\n","      <td>0.144356</td>\n","    </tr>\n","    <tr>\n","      <th>Motorcar_Parts_America</th>\n","      <td>-0.035608</td>\n","      <td>0.124629</td>\n","      <td>0.160237</td>\n","      <td>0.715134</td>\n","      <td>0.163343</td>\n","    </tr>\n","    <tr>\n","      <th>MS</th>\n","      <td>-0.095327</td>\n","      <td>0.037383</td>\n","      <td>0.13271</td>\n","      <td>0.829907</td>\n","      <td>0.112245</td>\n","    </tr>\n","    <tr>\n","      <th>Silicon</th>\n","      <td>-0.047157</td>\n","      <td>0.047157</td>\n","      <td>0.094313</td>\n","      <td>0.85853</td>\n","      <td>0.125061</td>\n","    </tr>\n","    <tr>\n","      <th>JPMorgan</th>\n","      <td>-0.107865</td>\n","      <td>0.035955</td>\n","      <td>0.14382</td>\n","      <td>0.820225</td>\n","      <td>0.094653</td>\n","    </tr>\n","    <tr>\n","      <th>CityGroup</th>\n","      <td>-0.039683</td>\n","      <td>0.056689</td>\n","      <td>0.096372</td>\n","      <td>0.846939</td>\n","      <td>0.089598</td>\n","    </tr>\n","    <tr>\n","      <th>BankofAmerica</th>\n","      <td>0.018868</td>\n","      <td>0.018868</td>\n","      <td>0.0</td>\n","      <td>0.981132</td>\n","      <td>0.129032</td>\n","    </tr>\n","    <tr>\n","      <th>Chevron</th>\n","      <td>0.048689</td>\n","      <td>0.153558</td>\n","      <td>0.104869</td>\n","      <td>0.741573</td>\n","      <td>0.094554</td>\n","    </tr>\n","    <tr>\n","      <th>Conoco</th>\n","      <td>-0.005797</td>\n","      <td>0.150725</td>\n","      <td>0.156522</td>\n","      <td>0.692754</td>\n","      <td>0.09016</td>\n","    </tr>\n","    <tr>\n","      <th>General_Electric</th>\n","      <td>-0.049689</td>\n","      <td>0.120083</td>\n","      <td>0.169772</td>\n","      <td>0.710145</td>\n","      <td>0.163008</td>\n","    </tr>\n","    <tr>\n","      <th>Petrogas</th>\n","      <td>-0.283333</td>\n","      <td>0.05</td>\n","      <td>0.333333</td>\n","      <td>0.616667</td>\n","      <td>0.086592</td>\n","    </tr>\n","    <tr>\n","      <th>Royal_Energy</th>\n","      <td>-0.095745</td>\n","      <td>0.031915</td>\n","      <td>0.12766</td>\n","      <td>0.840426</td>\n","      <td>0.077168</td>\n","    </tr>\n","    <tr>\n","      <th>Beyond_Meat</th>\n","      <td>-0.157658</td>\n","      <td>0.204955</td>\n","      <td>0.362613</td>\n","      <td>0.432432</td>\n","      <td>0.12574</td>\n","    </tr>\n","    <tr>\n","      <th>Campbell</th>\n","      <td>-0.093023</td>\n","      <td>0.136628</td>\n","      <td>0.229651</td>\n","      <td>0.633721</td>\n","      <td>0.156427</td>\n","    </tr>\n","    <tr>\n","      <th>Hershey</th>\n","      <td>0.134454</td>\n","      <td>0.266106</td>\n","      <td>0.131653</td>\n","      <td>0.602241</td>\n","      <td>0.126217</td>\n","    </tr>\n","    <tr>\n","      <th>Kellogg</th>\n","      <td>-0.09761</td>\n","      <td>0.165339</td>\n","      <td>0.262948</td>\n","      <td>0.571713</td>\n","      <td>0.15163</td>\n","    </tr>\n","    <tr>\n","      <th>McDonalds</th>\n","      <td>0.071006</td>\n","      <td>0.239645</td>\n","      <td>0.168639</td>\n","      <td>0.591716</td>\n","      <td>0.118626</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                        average_compound  perc_pos  perc_neg perc_neut  \\\n","Ford                            -0.07393  0.107004  0.180934  0.712062   \n","Tesla                          -0.005988  0.164671  0.170659  0.664671   \n","Standard_Motor_Products        -0.024814  0.133995  0.158809  0.707196   \n","Rivian                         -0.131737  0.155689  0.287425  0.556886   \n","Motorcar_Parts_America         -0.035608  0.124629  0.160237  0.715134   \n","MS                             -0.095327  0.037383   0.13271  0.829907   \n","Silicon                        -0.047157  0.047157  0.094313   0.85853   \n","JPMorgan                       -0.107865  0.035955   0.14382  0.820225   \n","CityGroup                      -0.039683  0.056689  0.096372  0.846939   \n","BankofAmerica                   0.018868  0.018868       0.0  0.981132   \n","Chevron                         0.048689  0.153558  0.104869  0.741573   \n","Conoco                         -0.005797  0.150725  0.156522  0.692754   \n","General_Electric               -0.049689  0.120083  0.169772  0.710145   \n","Petrogas                       -0.283333      0.05  0.333333  0.616667   \n","Royal_Energy                   -0.095745  0.031915   0.12766  0.840426   \n","Beyond_Meat                    -0.157658  0.204955  0.362613  0.432432   \n","Campbell                       -0.093023  0.136628  0.229651  0.633721   \n","Hershey                         0.134454  0.266106  0.131653  0.602241   \n","Kellogg                         -0.09761  0.165339  0.262948  0.571713   \n","McDonalds                       0.071006  0.239645  0.168639  0.591716   \n","\n","                        perc_esg_phrases  \n","Ford                            0.110452  \n","Tesla                           0.118833  \n","Standard_Motor_Products         0.158033  \n","Rivian                          0.144356  \n","Motorcar_Parts_America          0.163343  \n","MS                              0.112245  \n","Silicon                         0.125061  \n","JPMorgan                        0.094653  \n","CityGroup                       0.089598  \n","BankofAmerica                   0.129032  \n","Chevron                         0.094554  \n","Conoco                           0.09016  \n","General_Electric                0.163008  \n","Petrogas                        0.086592  \n","Royal_Energy                    0.077168  \n","Beyond_Meat                      0.12574  \n","Campbell                        0.156427  \n","Hershey                         0.126217  \n","Kellogg                          0.15163  \n","McDonalds                       0.118626  "]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["df_phrases"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ds4JjcLLM_5h"},"outputs":[],"source":["df_phrases.to_csv('Sentimental_Bert_on_10K.csv')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}